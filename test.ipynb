{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from SumoConnection import SumoConnection\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import traci\n",
    "import gymnasium as gym\n",
    "from numpy import inf \n",
    "import numpy as np\n",
    "from Sensors import getSumoSensors_full,len_sensors,len_optimized_sensors\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "import SumoEnv\n",
    "from config import *\n",
    "from Utils import show_ppo_progress\n",
    "from Utils_running import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"keys.env\")\n",
    "sumo_home = str(os.getenv(\"sumo_home\"))\n",
    "sumo_binary = str(os.getenv(\"sumo_binary\"))\n",
    "path_cfg = str(os.getenv(\"path_cfg\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumo_binary = \"C:/Program Files (x86)/Eclipse/Sumo/bin/sumo.exe\"\n",
    "cmd=[sumo_binary, \"-c\", path_cfg, \"--log\", \"sumo_log.txt\", \"--verbose\", \"true\"]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "SumoConnection(cmd).reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_lights,policies=get_traffic_lights_policies_high_group(20)\n",
    "#get_traffic_lights_policies_high_group(20)#get_traffic_lights_policies_high_group(60)#get_traffic_lights_policies_full(30,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 30\n",
    "n_epsiode = 5\n",
    "max_sumo_steps = max_steps#n_epsiode*max_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch_divide = 1\n",
    "n_mini_batch_divide = 1\n",
    "num_epochs = 4 # how often it should go through the whole dataset\n",
    "\n",
    "train_batch_size = max_steps // n_batch_divide # how much data it should use to train\n",
    "minibatch_size = minibatch_size = max(1, train_batch_size // n_mini_batch_divide) # how much  minibatch size it train with\n",
    "#confliction between stpes in epsiodes is normal with RLLib if small\n",
    "\n",
    "## reset=True, means that the environment will be reset after each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import SumoEnv  # Import your module\n",
    "import importlib  # Import importlib\n",
    "import rewards\n",
    "\n",
    "importlib.reload(SumoEnv)  # Reload the module after modification\n",
    "importlib.reload(rewards)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "register_env(\"GroupedSumoEnv\", lambda config: SumoEnv.GroupedSumoEnv(cmd,traffic_lights,True,max_steps=max_steps,max_sumo_steps=max_sumo_steps))\n",
    "register_env(\"SumoEnv\", lambda config: SumoEnv.SumoEnv(cmd,traffic_lights,True,max_steps=max_steps,max_sumo_steps=max_sumo_steps))\n",
    "register_env(\"HighGroupedSumoEnv\", lambda config: SumoEnv.HighGroupedSumoEnv(cmd,traffic_lights,True,max_steps=max_steps,max_sumo_steps=max_sumo_steps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=[]\n",
    "reward_ls=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "# Initialize Ray\n",
    "#ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "ENV_NAME = \"SumoEnv\"  # or \"HighGroupedSumoEnv\" or \"GroupedSumoEnv\"\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna objective function to optimize RLlib PPO\"\"\"\n",
    "    ray.init(ignore_reinit_error=True)\n",
    "\n",
    "    print(\"Trial:\", trial.number)\n",
    "    # Define hyperparameter search space\n",
    "    config = (\n",
    "        PPOConfig()\n",
    "        .environment(ENV_NAME)\n",
    "        .framework(\"torch\")  # or \"tf\"\n",
    "        .multi_agent(\n",
    "            policies=policies,  # Define policies\n",
    "            policy_mapping_fn=lambda agent_id, episode, **kwargs: agent_id,\n",
    "        )\n",
    "        .training(\n",
    "            lr=trial.suggest_loguniform(\"lr\", 1e-5, 1e-2),\n",
    "            gamma=trial.suggest_uniform(\"gamma\", 0.8, 0.9999),\n",
    "            lambda_=trial.suggest_uniform(\"lambda\", 0.8, 1.0),\n",
    "            clip_param=trial.suggest_uniform(\"clip_param\", 0.1, 0.4),\n",
    "            train_batch_size=trial.suggest_int(\"train_batch_size\", 1000, 4000, step=500),\n",
    "        )  \n",
    "        .env_runners(num_env_runners=0) # Set the number of environment runners to 0”\n",
    "        .api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)  # Use old API stack for now\n",
    "\n",
    "    )\n",
    "\n",
    "    # Train using RLlib\n",
    "    algo = config.build_algo()\n",
    "    results = algo.train()\n",
    "    cum_loss, cum_reward=show_ppo_progress(0, results, show_each_agent=False)\n",
    "\n",
    "    algo.stop()\n",
    "    ray.shutdown()\n",
    "    return cum_reward  # Higher reward is better\n",
    "\n",
    "\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "# Print best hyperparameters\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Configure PPO\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"SumoEnv\")\n",
    "    .multi_agent(\n",
    "        policies=policies,\n",
    "        policy_mapping_fn=lambda agent_id, episode, **kwargs: agent_id,\n",
    "    )\n",
    "    .training(\n",
    "        minibatch_size=minibatch_size, \n",
    "        train_batch_size=train_batch_size,\n",
    "        num_epochs=num_epochs,\n",
    "        lr=1e-4,\n",
    "        gamma=0.95,\n",
    "        lambda_=0.95,\n",
    "        clip_param=0.2,\n",
    "        vf_clip_param=10.0,\n",
    "        entropy_coeff=0.01,\n",
    "        kl_target=0.01,\n",
    "        kl_coeff=0.5,\n",
    "    )\n",
    "    .env_runners(num_env_runners=0) # Set the number of environment runners to 0”\n",
    "    .api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)  # Use old API stack for now\n",
    ")\n",
    "\n",
    "    \n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Build the PPO trainer\n",
    "trainer = config.build_algo()\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for i in range(n_epsiode):### remind in step it end after 1000 (max i put)\n",
    "    result = trainer.train()\n",
    "    policy_rewards = result.get('env_runners', {}).get('policy_reward_mean', {})\n",
    "    print(f\"Episode {i+1}: {policy_rewards}\")\n",
    "    cum_loss, cum_reward=show_ppo_progress(i+1, result, show_each_agent=False)\n",
    "    loss.append(cum_loss)\n",
    "    reward_ls.append(cum_reward)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss, label=\"Loss\")\n",
    "plt.plot(reward_ls, label=\"Reward\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.title(\"Training Progress\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "traci.close()\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#D3QN (Dueling DDQN)\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "from ray.rllib.utils.replay_buffers import MultiAgentPrioritizedReplayBuffer\n",
    "\n",
    "# Configure DDQN\n",
    "config = (\n",
    "    DQNConfig()\n",
    "    .environment(\"SumoEnv\")\n",
    "    .multi_agent(\n",
    "        policies=policies,\n",
    "        policy_mapping_fn=lambda agent_id, episode, **kwargs: agent_id,\n",
    "    )\n",
    "    .training(\n",
    "        gamma=0.99,\n",
    "        lr=1e-3,\n",
    "        minibatch_size=minibatch_size, \n",
    "        train_batch_size=train_batch_size,\n",
    "        num_epochs=num_epochs,\n",
    "        target_network_update_freq=8000,\n",
    "        replay_buffer_config={\n",
    "            \"type\": MultiAgentPrioritizedReplayBuffer,\n",
    "            \"capacity\": 50000,\n",
    "        },\n",
    "        # Enable double Q-learning\n",
    "        double_q=True,\n",
    "        dueling=True,\n",
    "        hiddens=[256, 256],\n",
    "        #prioritized_replay=True,\n",
    "        #prioritized_replay_alpha=0.6,\n",
    "        #prioritized_replay_beta=0.4,\n",
    "        #final_prioritized_replay_beta=1.0,\n",
    "        #prioritized_replay_eps=1e-6,\n",
    "    )\n",
    "    .env_runners(num_env_runners=0)  # Set the number of environment runners to 0\n",
    "    #.rollouts(num_rollout_workers=0)  # No rollout workers when running locally\n",
    "    # If using the old API stack:\n",
    "    .api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)\n",
    ")\n",
    "\n",
    "ray.init(ignore_reinit_error=True) \n",
    "\n",
    "# Build the DQN trainer\n",
    "trainer = config.build_algo()\n",
    "\n",
    "# Training loop\n",
    "loss = []\n",
    "rewards = []\n",
    "for i in range(n_epsiode):### remind in step it end after 1000 (max i put)\n",
    "    result = trainer.train()\n",
    "    print(result)\n",
    "    policy_rewards = result.get('env_runners', {}).get('policy_reward_mean', {})\n",
    "    print(f\"Episode {i+1}: {policy_rewards}\")\n",
    "    #cum_loss, cum_reward=show_ppo_progress(i+1, result, show_each_agent=False)\n",
    "    loss.append(cum_loss)\n",
    "    reward_ls.append(cum_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
